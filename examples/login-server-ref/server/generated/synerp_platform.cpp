/**
 * FILE: synerp_platform.cpp
 * --------------------------
 * Architecture: 
 * - Multithreaded, single kernel struct sock per interface
 * that is being watched by multiple epoll event fds (one per thread)
 * - On event detection, all sleeping threads wake up, but only a single thread
 * accepts the connection.
 * - All TCP connections are SHORT to ensure receipt of single datagram per connection.
 * - Server initiates the connection close, hence there is a tendency for large numbers
 * of TIME_WAIT sockets 
 * 
 * this NF as a server: Accepts connection requests from client at port local_port
 * - client connect() will allocate a port for the connection if not bound before connect.
 * this NF as a client: sends responses on previously created sockets before closing the connection.
 * 
 * NOTE: At initialisation, this NF only accepts connections/data (i.e. as a Server). Only in subsequent procedure flows does this 
 * NF begin to initiate connections (i.e. as a client) to other NFs.
*/

// all are defaults
#include "synerp_platform.h"
#include "../../utility_library/platform/include/logging.h"
#include <vector>
#include <pthread.h>
#include <errno.h>
#include <fcntl.h>
#include <sys/socket.h>
#include <bits/stdc++.h>
#include <netinet/sctp.h>

std::string _errno(const std::string& base) {
    return base + ": " + std::strerror(errno);
}

// declare server entry point
void server_entry(std::vector<char>& message, int length, int sockfd, struct nfvInstanceData *nfvInst);

// To track nf instances in a multithreaded setup
std::vector<struct nfvInstanceData *> nfvInstVector;

// SCTP payload protocol params
#define SCTP_PPID_NGAP 1006632960L /* htonl(60) */
#define SCTP_PPID_S1AP 301989888L  /* htons(18) */
uint8_t ppid_warn_rate = 0;        /* will print a warning every 20th time */

// user-level locks
pthread_mutex_t user_login_map_lock = PTHREAD_MUTEX_INITIALIZER;

// platform-level locks
pthread_mutex_t fd_request_handler_map_lock = PTHREAD_MUTEX_INITIALIZER;
pthread_mutex_t fd_response_handler_map_lock = PTHREAD_MUTEX_INITIALIZER;
pthread_mutex_t port_request_handler_map_lock = PTHREAD_MUTEX_INITIALIZER;

pthread_mutex_t nfvInst_vector_lock = PTHREAD_MUTEX_INITIALIZER;
pthread_mutex_t fd_to_key_map_lock = PTHREAD_MUTEX_INITIALIZER;
pthread_mutex_t key_to_fd_map_lock = PTHREAD_MUTEX_INITIALIZER;
pthread_mutex_t persistent_conn_key_to_fd_map_lock = PTHREAD_MUTEX_INITIALIZER;

// platform-level maps
std::map<int, void(*)(std::vector<char>&, int, int, struct nfvInstanceData *)> fd_request_handler_map;
std::map<int, void(*)(std::vector<char>&, int, int, struct nfvInstanceData *)> fd_response_handler_map;
std::map<int, void(*)(std::vector<char>&, int, int, struct nfvInstanceData *)> port_request_handler_map;
std::map<int, int> fd_to_key_map;
std::map<int, int> key_to_fd_map;
std::map<int, int> persistent_conn_key_to_fd_map;
// define interfaces @server
// generated by translator from arch.json
std::vector<fdData_t> interfaceVector = {(fdData_t){UDP_PROTOCOL_SERVER_ACCEPT_SOCKET, 5858,0,INADDR_NONE,SHORT}};


// init the NF instance struct
struct nfvInstanceData *init_platform(uint32_t ip_addr) {
    struct nfvInstanceData *nfvInst = new struct nfvInstanceData;
    nfvInst->state = E_NFV_INVALID_STATE;
    nfvInst->fd_map = {};
    nfvInst->fd_to_be_closed_set = {};
    nfvInst->epoll_fd = -1;
    nfvInst->bind_addr = ip_addr; // bound to all interfaces.
    return nfvInst;
}

// mark socket as non-blocking, ensuring
// other flags are preserved.
// Client socket shouldnt be non-blocking, as connect() cant directly
// confirm the result of server accept().   
int set_non_block(int fd) {
    LOG_ENTRY;
    int flags;
    // std::cout << "nonbb" << "\n";

    if ((flags = fcntl(fd, F_GETFL, 0)) == -1) {
        lowLog("fcntl get flags failed on fd %d, Error %s\n", fd, strerror(errno));
        LOG_EXIT;
        return -1;
    }

    if (fcntl(fd, F_SETFL, flags | O_NONBLOCK) == -1) {
        lowLog("fcntl set flags failed on fd %d, Error %s\n", fd, strerror(errno));
        LOG_EXIT;
        return -1;
    }
    LOG_EXIT;
    return 0;
}

// setup server interface i.e. create listen sockets
int open_channels(struct nfvInstanceData *nfvInst) {
    LOG_ENTRY;
    for (auto interface: interfaceVector) {
        fdData_t socket_fdd = interface;
        // std::cout <<"here" <<"\n";

        // init socket for each interface
        if (open_and_init_socket(socket_fdd, nfvInst->bind_addr, nfvInst) < 0) {
            lowLog("ERROR in open and init socket type %d\n", socket_fdd.type);
        }
    }

    LOG_EXIT;
    return SUCCESS;
}


// creates a kernel socket object according to provided
// interface details in the socket_fdd. These are the listen
// sockets for tcp, and data sockets for udp/sctp.
// ============================================================
// SO_REUSEADDR: 
// Affects bind() semantics. 
// Server never closes the listenfds, hence listenfds are in TIME_WAIT state only
// when the server process is killed. 
// SO_REUSEADDR allows a socket to bind to a port that is associated with a 
// socket in TIME_WAIT state.
// Here, SO_REUSEADDR ensures that the same port number can be reused immediately
// to BIND to the server listen socket in subsequent procedure runs.
//--------------------------------
// SO_REUSEPORT: load-balanced datagram distribution across listening threads when 
// a single listen-socket is shared across sleeping threads.
// This also affects bind() semantics (i.e. allow bind() to TW sockets given certain condns)
// allow multiple sockets to bind to the same port. Kernel will load balance incoming messages to
// different socket instances.
// -------------------------------
// tcp_tw_reuse: Essential, since the accept() data sockets arent being
// SO_REUSEADDR'd, hence the time wait sockets will keep rising. 
int open_and_init_socket(fdData_t& socket_fdd, uint32_t bind_addr, struct nfvInstanceData *nfvInst) {
    LOG_ENTRY;
    int fd {};
    uint local_port = socket_fdd.port; // from interface config.
    struct sockaddr_in local_addr {};

    if (socket_fdd.type == TCP_PROTOCOL_SERVER_ACCEPT_SOCKET) {
        if ((fd = socket(AF_INET, SOCK_STREAM, 0)) == -1) {
            lowLog("TCP SERVER create failed, port %d, Error %s\n",
                   local_port, strerror(errno));
            LOG_EXIT;
            return -1;
        }
        local_addr.sin_family = AF_INET;
        local_addr.sin_port = htons(local_port);
        local_addr.sin_addr.s_addr = bind_addr;
        
        int tr = 1;
        if (setsockopt(fd, SOL_SOCKET, SO_REUSEADDR, &tr, sizeof(int)) == -1) {
            lowLog("Port reuse failed, port %d, Error %s\n", local_port, strerror(errno));
            LOG_EXIT;
            return -1;
        }
    } else if (socket_fdd.type == SCTP_PROTOCOL_SERVER_ACCEPT_SOCKET) {
        if ((fd = socket(AF_INET, SOCK_STREAM, IPPROTO_SCTP)) == -1) {
            lowLog("SCTP SERVER create failed, port %d, Error %s\n",
                   local_port, strerror(errno));
            LOG_EXIT;
            return -1;
        }
        local_addr.sin_family = AF_INET;
        local_addr.sin_port = htons(local_port);
        local_addr.sin_addr.s_addr = bind_addr;

        // SCTP defines an "association" (i.e. generalised connection) 
        // between two devices. Each device has one-one (~TCP) or 
        // one-to-many (multihoming) interfaces (IP, Port) for communication.
        //
        // Sockets are the connection endpoints for a SCTP stream.
        // Eg: one socket to many other sockets, each communicating via 
        // a unique/independent stream identified by a socket associationID.
        // Contains attributes for a new association created by send.
        struct sctp_initmsg init_msg {};

        init_msg.sinit_num_ostreams = 2;
        init_msg.sinit_max_instreams = 100;
        lowLog("Setting %d instreams and atmost %d outstreams", initmsg.sinit_num_ostreams, initmsg.sinit_max_instreams);
        
        if (setsockopt(fd, SOL_SCTP, SCTP_INITMSG, &init_msg, sizeof(init_msg)) == -1) {
            lowLog("setsockopt failed, Error: %s\n", strerror(errno));
            LOG_EXIT;
            return -1;
        }
        
        // enable receipt of stream id on message receipt at the sctp socket.
        // via an sctp event.
        struct sctp_event_subscribe events = {};
        events.sctp_data_io_event = 1;
        if (setsockopt(fd, SOL_SCTP, SCTP_EVENTS, (const void *)&events, sizeof(events)) == -1)
        {
            lowLog("setsockopt failed, Error: %s\n", strerror(errno));
            LOG_EXIT;
            return -1;
        }

        int tr = 1;
        if (setsockopt(fd, SOL_SOCKET, SO_REUSEADDR, &tr, sizeof(int)) == -1)
        {
            lowLog("Port reuse failed, port %d, Error %s", local_port, strerror(errno));
            LOG_EXIT;
            return -1;
        }
    } else if (socket_fdd.type == UDP_PROTOCOL_SERVER_ACCEPT_SOCKET) {
        if ((fd = socket(AF_INET, SOCK_DGRAM, 0)) == -1) {
            lowLog("UDP SERVER create failed, port %d, Error %s\n",
                   local_port, strerror(errno));
            LOG_EXIT;
            return -1;
        }
        local_addr.sin_family = AF_INET;
        local_addr.sin_port = htons(local_port);
        local_addr.sin_addr.s_addr = bind_addr;
        // std::cout << "made it" << local_port << "\n";
        
        int tr = 1;
        if (setsockopt(fd, SOL_SOCKET, SO_REUSEADDR, &tr, sizeof(int)) == -1) {
            lowLog("Port reuse failed, port %d, Error %s", local_port, strerror(errno));
            LOG_EXIT;
            return -1;
        }  
        // register a handler with the platform file, specifically for UDP
        fd_request_handler_map[fd] = port_request_handler_map[local_port];
    } else {
        lowLog("Unknown socket type %d\n", socket_fdd.type);
        LOG_EXIT;
        return -1;
    }

    if (bind(fd, (struct sockaddr *)&local_addr, sizeof(local_addr)) == -1) {
        // std::cout << "filaed" << strerror(errno) << "\n";
        lowLog("Bind failed, port %d, Error %s", local_port, strerror(errno));
        LOG_EXIT;
        return -1;
    }

    if (socket_fdd.type != UDP_PROTOCOL_SERVER_ACCEPT_SOCKET) {
        set_non_block(fd);
        if (listen(fd, LISTEN_QUEUE_BACKLOG) == -1) {
            lowLog("Listen failed, port %d, Error %s", local_port, strerror(errno));
            close(fd);
            LOG_EXIT;
            return -1;
        }
    }

    // add the listening sockets to the fd_map
    // of the input nfvInst (subsequently copied to rest)
    socket_fdd.id = fd;
    socket_fdd.fd = fd;
    nfvInst->fd_map[socket_fdd.id] = socket_fdd;
    //std::cout << nfvInst->fd_map.size() << "\n";

    LOG_EXIT;
    return fd;
}


// Create data socket and add it to the epoll fd watch set.
// Note that there is a potential data race due to the wakeup of
// multiple sleeping threads on epoll_wait event detection.
// ---------------------------
// Eventually, the message handler for the new data socket is registered into 
// the global fd_request_handler_map.
void accept_connections(fdData_t *listen_socket_fdd, struct nfvInstanceData *nfvInst) {
    LOG_ENTRY;
    
    // spawn data socket. need to add its socket struct to active_fd map.
    // and epfd watch set.
    lowLog("wtf am i doin her\n");
    int new_fd = accept4(listen_socket_fdd->fd, NULL, NULL, SOCK_NONBLOCK);
    if (new_fd == -1) {
        lowLog("Accept4 failed, error %s\n", strerror(errno));
        LOG_EXIT;
        return;
    }

    if (nfvInst->fd_map.size() >= MAX_CONNECTIONS)
    {
        lowLog("%s\n", "fd map full");
        close(new_fd); /*refuse connection */
        LOG_EXIT;
        return;
    }

    // Create the socket struct for the newly accepted connection
    fdData_t new_socket_fdd {};
    switch (listen_socket_fdd->type) {
        case SCTP_PROTOCOL_SERVER_ACCEPT_SOCKET:
            new_socket_fdd.type = SCTP_PROTOCOL_SERVER_DATA_SOCKET;
            break;
        case TCP_PROTOCOL_SERVER_ACCEPT_SOCKET:
            new_socket_fdd.type = TCP_PROTOCOL_SERVER_DATA_SOCKET;
            break;
        default:
            lowLog("Event on unknown accept socket\n");
            break;
    }
    new_socket_fdd.fd = new_fd;
    new_socket_fdd.id = new_fd;
    new_socket_fdd.port = listen_socket_fdd->port;

    // OS and user-app may have different views of available fds.
    // hence it is conceivable that OS assigns an fd to a connection
    // that is not yet erased from the active_fd tracking map.
    // this indicates an error in map coherence.
    if (nfvInst->fd_map.find(new_socket_fdd.id) != nfvInst->fd_map.end()) {
        lowLog("Error while adding fd into the map\n");
        close(new_fd);
        int erasedFds = nfvInst->fd_map.erase(new_fd);
        lowLog("Erased %d fds\n", erasedFds);
        LOG_EXIT;
        return;
    }

    // There are two kinds of connections that could be specified by 
    // the interface: persistent and short.
    // persistent connection state must be shared across all threads.
    // One thread holds the nfinstvector lock, hence datarace terminates here
    // for a persistent connection.
    if (listen_socket_fdd->connectionType == PERSISTENT) {
        pthread_mutex_lock(&nfvInst_vector_lock);
        for (size_t i = 0; i < nfvInstVector.size(); i++) {
            // register this as active socket in every thread.
            nfvInstVector[i]->fd_map[new_socket_fdd.id] = new_socket_fdd;

            // add this persistent connection to the watch set of every thread.
            struct epoll_event event {};
            event.events = EPOLLIN;
            event.data.fd = new_fd;
            int epfd = nfvInstVector[i]->epoll_fd;
            if (epoll_ctl(epfd, EPOLL_CTL_ADD, new_fd, &event) == -1) {
                lowLog("epoll_ctl failed, fd id %d, Error %s", new_socket_fdd.id, strerror(errno));
                LOG_EXIT;
                return;
            }
            midLog("Accepted a new connection, epoll %d, fd %d, core %d\n", epfd, fdd.fd, nfvInstVector[i]->threadId); /* LNTC */
        }
        pthread_mutex_lock(&nfvInst_vector_lock);
    } else {
        // SHORT Connections: One connection per request.
        // it is possible that multiple threads record the same socket
        // fdd. Here, a situation may arise such that two epoll instances
        // track the same data socket.
        nfvInst->fd_map[new_socket_fdd.id] = new_socket_fdd;

        struct epoll_event event {};
        event.events = EPOLLIN;
        event.data.fd = new_fd;
        int epfd = nfvInst->epoll_fd;
        if (epoll_ctl(epfd, EPOLL_CTL_ADD, new_fd, &event) == -1)
        {
            lowLog("epoll_ctl failed, fd id %d, Error %s\n", new_socket_fdd.id, strerror(errno));
            LOG_EXIT;
            return;
        }
        lowLog("Accepted a new connection, id %d, fd %d\n", new_socket_fdd.id, new_socket_fdd.fd);
    }
    
    // server knows that some fd has been allocated by OS to serve
    // the newly accepted connection. Hence, can register the request handler
    pthread_mutex_lock(&fd_request_handler_map_lock);
    fd_request_handler_map[new_socket_fdd.fd] = port_request_handler_map[new_socket_fdd.port];
    pthread_mutex_unlock(&fd_request_handler_map_lock);

    LOG_EXIT;
}


void recv_and_process_callbacks(struct nfvInstanceData *nfvInst, std::map<uint, fdData_t>::iterator& recv_socket_fdd_itr) {
    fdData_t& recv_socket_fdd = recv_socket_fdd_itr->second;

    std::vector<char> msg(MAX_MESSAGE_SIZE);

    // Based on the type of socket that received a read i.e. EPOLLIN event, 
    // the appropriate read function will be called to read the message
    // contents into a char buffer. This message is passed to the linking file
    // for further processing.
    // -----------------------
    // Note: TCP only recognises json headers.
    int stream_id, rc = 0;
    switch (recv_socket_fdd.type) {
    case SCTP_PROTOCOL_SERVER_DATA_SOCKET:
        rc = platform_sctp_recv_data(recv_socket_fdd.fd, msg, &stream_id);
        break;
    case TCP_PROTOCOL_SERVER_DATA_SOCKET:
        rc = platform_tcp_recv_data(recv_socket_fdd.fd, msg);
        break;
    case UDP_PROTOCOL_SERVER_DATA_SOCKET:
        rc = platform_udp_recv_data(recv_socket_fdd.fd, msg);
        break;
    case SCTP_PROTOCOL_CLIENT_SOCKET:
        rc = platform_sctp_recv_data(recv_socket_fdd.fd, msg, &stream_id);
        break;
    case TCP_PROTOCOL_CLIENT_SOCKET:
        rc = platform_tcp_recv_data(recv_socket_fdd.fd, msg);
        break;
    case UDP_PROTOCOL_CLIENT_SOCKET:
        rc = platform_udp_recv_data(recv_socket_fdd.fd, msg);
        break;
    default:
        lowLog("no action needed for recv_socket_fdd.type %d", recv_socket_fdd.type);
        LOG_EXIT;
        return; // vector will be freed
    }

    // The recv functions ensure that a full message has been read. 
    // Therefore, we can continue.
    // ---------------------------
    // Control now enters the linking.cpp file, and the code
    // only re-enters platform.cpp upon a send_data() from linking.cpp
    if (rc > 0) {
        pthread_mutex_lock(&fd_request_handler_map_lock);
        // this map is updated whenever a new connection is accepted by the server.
        // success here implies server is receiving a request from some other NF after
        // that NF had connected to it.
        auto active_socket_itr = fd_request_handler_map.find(recv_socket_fdd.fd);
        if (active_socket_itr == fd_request_handler_map.end()) {
            // The socket that just received a read event was not created by the 
            // server as a result to a connection request (i.e. via accept_conn())
            // the socket was probably created by the server with random port and
            // ip in order to respond to another NF. These socket fd are stored in
            // fd_response_handler_map. In this case, the NF is acting as a Client
            // handling a server response to a previously sent request (over a 
            // previously initiated connection by this NF)
            active_socket_itr = fd_response_handler_map.find(recv_socket_fdd.fd);
            
            if (active_socket_itr == fd_response_handler_map.end()) {
                // the socket that recd the message was not being tracked by
                // the application. No handler was registered.
                pthread_mutex_unlock(&fd_request_handler_map_lock);
                lowLog("Handler couldn't be found for the fd:%d\n", recv_socket_fdd.fd);
            } else {
                // message is a response, recd on a socket made via a 
                // this NF connect(). 
                pthread_mutex_unlock(&fd_request_handler_map_lock);
                // call the processing entry function stored in 
                // fd_response_handler_map.
                active_socket_itr->second(msg, rc, recv_socket_fdd.fd, nfvInst);
            }
        } else {
            // message is a request from another NF, recd on a socket made by 
            // server accept().
            pthread_mutex_unlock(&fd_request_handler_map_lock);
            // Call the processing entry function
            // stored in the fd_request_handler_map
            active_socket_itr->second(msg, rc, recv_socket_fdd.fd, nfvInst);
        }
    } else if (rc == 0) {
        // zero-length read implies the connection was closed by the 
        // client. Stop monitoring that socket. Cleanup the map that registered
        // the socket fd with message handlers.
        int epfd = nfvInst->epoll_fd;
        remove_fd_from_epoll(epfd, recv_socket_fdd.fd);
        if (close(recv_socket_fdd.fd) == -1) {
            lowLog("close failed, Error %s\n", strerror(errno));
        }
        /* TODO: notify that the connection has closed. */
        midLog("Removed data fd %d, id%d", recv_socket_fdd.fd, recv_socket_fdd.id);
        fd_request_handler_map.erase(recv_socket_fdd.fd);
        recv_socket_fdd.fd = -1;
        nfvInst->fd_map.erase(recv_socket_fdd_itr);
    }
}

void inline remove_fd_from_epoll(int epfd, int fd)
{
    LOG_ENTRY;
    if (epoll_ctl(epfd, EPOLL_CTL_DEL, fd, NULL) == -1) {
        lowLog("epoll_ctl DEL failed, Error %s for fd :%d and epoll fd: %d\n", strerror(errno),fd, epfd);
    } else {
        midLog("Removed fd %d from epoll fd %d",fd, epfd);
    }
    LOG_EXIT;
}


// Implements the per-thread server event-loop. Triggers
// actions and maintains user-level active-fd state
// while the server is running.
void *poll_on_events(void *arg) {
    // Assign this thread to a single core
    struct nfvInstanceData *nfvInst = (struct nfvInstanceData *)arg;
    int thread_id = nfvInst->thread_id;
    cpu_set_t cpu_set;
    CPU_ZERO(&cpu_set);
    CPU_SET(2 + thread_id, &cpu_set); // 12 cores on local machine
    pthread_t this_thread = pthread_self();
    if (pthread_setaffinity_np(this_thread, sizeof(cpu_set), &cpu_set) != 0) {
        lowLog("Failed to set cpu affinity\n");
    }

    // Define an epoll-fd to watch for events
    struct epoll_event active_events[MAX_EPOLL_EVENTS];
    if ((nfvInst->epoll_fd = epoll_create1(0)) == -1) {
        lowLog("Epoll fd creation failed\n");
        exit(EXIT_FAILURE);
    }
    int epfd = nfvInst->epoll_fd;

    // Add the previously created tcp/sctp listen sockets + udp sockets of the 
    // NF interface to this thread's epoll fd watch set.
    // epfd of every server instance monitors the same listen sockets
    // for connection requests in case of tcp, and data in case of udp.
    // Any event at that socket would trigger multiple threads to wake up.
    for (auto fd_map_itr = nfvInst->fd_map.begin(); fd_map_itr != nfvInst->fd_map.end(); fd_map_itr++) {
        fdData_t *socket_fdd = &(fd_map_itr->second);
        struct epoll_event event {};

        event.events = EPOLLIN;
        event.data.fd = socket_fdd->fd;
        if (epoll_ctl(epfd, EPOLL_CTL_ADD, socket_fdd->fd, &event) == -1) {
            lowLog("epoll_ctl failed for listening Fd, Error %s\n", strerror(errno));
        } else {
            lowLog("added fd %d into epoll, type: %d\n", socket_fdd->fd, socket_fdd->type);
        }

        // Server marks the initial udp "listen" socket as a data
        // socket for further communications.
        if (socket_fdd->type == UDP_PROTOCOL_SERVER_ACCEPT_SOCKET) {
            socket_fdd->type = UDP_PROTOCOL_SERVER_DATA_SOCKET;
        }
        //lowLog("fd %d in map now has type: %d\n", socket_fdd->fd, nfvInst->fd_map[socket_fdd->fd].type);
    }

    // Start the server, i.e. the blocking epoll_wait event
    // loop in each thread.
    int num_fds {};
    nfvInst->state  = E_NFV_STARTED;
    for(;;) {
        // epoll-wait causes this thread to sleep (wait-queue) until its epfd detects 
        // an EPOLLIN event at a struct sock referenced via an fd in the epfd
        // watch set. The active_events array is populated with relevant event structs
        // ------------------------------
        // If mutiple threads are woken up at once, there will be a data race.
        // All awoken threads will enter accept()
        lowLog("Waiting....\n");
        num_fds = epoll_wait(epfd, active_events, MAX_EPOLL_EVENTS, -1);
        //lowLog("Got %d events, processing...\n", num_fds);

        // Indicate that this thread recd a wakeup via an epoll event.
        for (int wokeup = 0; wokeup < num_fds; wokeup++)
            lowLog("Event %d on fd %d woke up thread id %d\n", active_events[wokeup].events, active_events[wokeup].data.fd, thread_id);

        // Now that events have been detected at certain watched sockets, 
        // respond based on the type of socket that received the event.
        for (int i = 0; i < num_fds; i++) {
            uint event_fd = active_events[i].data.fd;

            // If an exception occurred at one of the monitored sockets, 
            // just remove it from the watch set and move to the next fd.
            if (active_events[i].events & EPOLLERR) {
                close(event_fd);
                epoll_ctl(nfvInst->epoll_fd, EPOLL_CTL_DEL, event_fd, NULL);
                continue;
            }

            // INVARIANT: Every fd that receives an event must
            // have been created and connected in the previous event-loop iteration 
            // (i.e. before the one that detected the current message arrival).
            // -----
            // Thus we have two possibilities:
            // a. event_fd is created at server init : interface listen fds
            // b. event_fd was created by an earlier request from server
            //    via a create_conn(), and is now receiving a response to its request 
            //    on the same connection.
            //    - Note that the close() for socket n (recv) occurs after socket 
            //      n has been used to send a response.
            //    - When message recd is a response from another NF, this NF
            //      sends a request. Here, the close for socket n (recv) occurs
            //      after socket n+1 has been created and used to send a request in
            //      response to the initial message from the peer.
            //      - Create n+1 -> add n+1 to to_be_closed set -> store n+1 in the 
            //        fd_map -> register n+1 with a response_handler i.e. callback to deal with 
            //        subsequent response from peer -> send request from socket n+1 -> close 
            //        socket n, REPEAT.
            //        NOTE: the sending socket at this NF will always be the one to receive
            //        the subsequent response
            //        * TCP, SCTP: socket-socket fixed connection, trivial.
            //        * UDP: connect() on a UDP socket at this NF will cause it to filter incoming
            //               messages by the pre-defined destination as source.
            // Remember that this NF will specify during send() whether the next message on
            // the sending socket has a callback or is a simple send.
            // If it is a simple send() response from UDP, the destination will have to be 
            // specified, as sockets are closed after every response send, thus dest
            // does not persist.
            auto socket_fdd_itr = nfvInst->fd_map.find(event_fd);
            if (socket_fdd_itr == nfvInst->fd_map.end()) {
                lowLog("ERROR: active_events not coherent, issue with cleanup.\n");
                continue;
            }

            if (socket_fdd_itr->second.type == SCTP_PROTOCOL_SERVER_ACCEPT_SOCKET ||
                socket_fdd_itr->second.type == TCP_PROTOCOL_SERVER_ACCEPT_SOCKET) {
                    // The very first messages to the server are connection requests
                    lowLog("udp bc\n");
                    accept_connections(&socket_fdd_itr->second, nfvInst);
            } else if (socket_fdd_itr->second.type == TIMERFD_SOCKET) {
                // The server epfd also monitors any application-level timers
                timer_expiry_context_t timer_ctx = socket_fdd_itr->second.timer_ctx;
                uint64_t c {};
                auto r = read(event_fd, &c, sizeof(c)); // consume event.
                lowLog("%ld timer expiries\n", r);
                socket_fdd_itr->second.timerCB(timer_ctx, nfvInst);
            } else {
                // The EPOLLIN event at the current socket corresponds to 
                // a data message receipt. accept_connections() ensures that 
                // the spawned data socket fd has the appropriate type
                // ---------------------------------------------------
                // all (UDP, TCP, SCTP) data sockets and client sockets.
                // On NF init (b4 p_o_e), UDP socket will be created to service data
                // messages and tracked in fd_map + have registered requesthandler.
                recv_and_process_callbacks(nfvInst, socket_fdd_itr);
            }

            // After a send_data() chain is complete, control returns to 
            // poll_on_events. Here, any sockets that are meant to be closed
            // are closed and removed from the epoll watch list.
            // -------------------------------------------------------------
            // Each NF instance thread maintains its own local set of 
            // sockets to be closed. The conditions for socket-close are:
            // a. Newly created (in current iter) connection socket is closed (in next loop iter) 
            //    after receiving a response and responding via cb.
            // b. If callback decays to simple response (via application-level processing logic),
            //    a simple send is performed via send_response() and control falls through to
            //    socket close() and fd_to_response_map + fd_map cleanup.
            // response_handler_map closed because a simple send_response() by definition
            // does not expect a response from peer.
            auto socket_to_be_closed_itr = nfvInst->fd_to_be_closed_set.find(event_fd);
            if (socket_to_be_closed_itr != nfvInst->fd_to_be_closed_set.end()) {
                remove_fd_from_epoll(nfvInst->epoll_fd, event_fd);
                if (close(event_fd) < 0) {
                    lowLog("Failed to close fd: %d\n", event_fd);
                    continue;
                }

                // map cleanup
                nfvInst->fd_to_be_closed_set.erase(event_fd);
                nfvInst->fd_map.erase(event_fd);

                pthread_mutex_lock(&fd_response_handler_map_lock);
                fd_response_handler_map.erase(event_fd);
                pthread_mutex_lock(&fd_response_handler_map_lock);

            }
        }
    }
}

// If a message-send for a particular procedure instance (identified by user-level key)
// has an entry in the user maintained key-to-fd map, infer that the current message-send 
// is in response to a previously received message, hence no callback would be expected 
// by this node either. The fd refers to a connected/udp data socket. key_or_fd -> fd.
// -------------------------------------------------------------------------------------
// Anytime this NF wants to initiate a request to a peer NF, it will initiate the
// socket creation + connection request procedure. The new socket then needs to 
// be associated with a callback function and the procedure-instance. Finally, it
// must be added to the epoll fd watch set to detect responses and trigger callbacks.
// -----------------------------------------------------------------------------------
// A single procedure call-flow involving this NF may involve multiple sends and recvs. To 
// identify flows, user-app assigns unique `key`. A single epoll fd may watch 
// multiple active sockets -> each active socket corresponds to a single procedure
// identifier (`key`). A crucial role of the platform file is to maintain
// the state necessary to enable this user-level demultiplexing behaviour.
// ---------------------------------------------------------------------------------------
// SHORT connection: The sending socket is closed as soon as it sends a message.
// Subsequent message sends will assign a new ephemeral port (if client) to 
// create a new socket. Receipt of message has no effect on connection-state
// of a socket. "New connection for each message send"
// 
// LONG/PERSISTENT connection: A single connection instance is made between two interfaces
// In high load conditions, there is less socket-creation and connection overhead, leading
// to better throughputs. Complexity will be added in the subsequent user-level message
// message framing -> mulitple/incomplete messages could be sent in a single epoll event.
//---------------------------------------------------------------------------------------
// send_data() is ONLY triggered in response to a message-receipt on this NF (via recv_and_process_cb)
// i.e. when a platform callback is triggered.
void send_data(std::string peer_ip, int peer_port, std::vector<char>& msg, _e_connectionType conn_type,
    _e_protocols protocol, NODE peer_node, size_t message_length, int procedure_key_or_original_receiver_fd, 
    void (*callback)(std::vector<char>&, int, int, nfvInstanceData *), struct nfvInstanceData *nfvInst
    ) {
    if (callback == NULL) {
        // previously recd a message -> a client had connected to
        // this NF previously and then sent its request -> this NF had
        // already created a data socket after accept() corresp. to
        // this client's initial request --> the socket will be in the 
        // activefds on this NF --> just send the response using the
        // fdmap instead of calling create_connection().
        send_response(peer_ip, peer_port, procedure_key_or_original_receiver_fd, msg, protocol, message_length); // fd
    } else {
        // this NF will have to create a new connection to 
        // the destination NF via create_connection().
        // create_conn() assigns a random port number to a new socket 
        // for this NF to send the request, and the new socket will 
        // be added to the ctive_sockets and depoll watch list to detect a response.
        // for udp socket, a default destination is established.
        send_new_request(peer_ip, peer_port, msg, conn_type, 
            protocol, peer_node, message_length, procedure_key_or_original_receiver_fd,
            callback, nfvInst);
    }

}

// sending a response assumes a request had been sent by this NF previously.
// but this is not strictly a safe assumption. 
// ------------------------------------------
// Consider the edge case of a simple multitier system that implements a UDP echo request
// response. On receipt of the first message at this NF udp socket, no default response destination has
// yet been established. New peer_addr will have to be generated.
int send_response(std::string peer_ip, int peer_port, int original_receiver_fd, std::vector<char>& msg, 
    _e_protocols protocol, int message_length
    ) {
    int ret;
    if (protocol == SCTP_PROTOCOL)
        ret = platform_sctp_send_data(original_receiver_fd, msg, message_length, NULL, 0);
    else if (protocol == TCP_PROTOCOL)
        ret = platform_tcp_send_data(original_receiver_fd, msg, message_length, NULL);
    else if (protocol == UDP_PROTOCOL) {
        // issue: sending a simple response on a UDP socket
        // that does not have a default destination.
        // Fix: Ensure that all udp SENDs are passed the sockaddr struct.
        struct sockaddr_in peer_addr {};
        peer_addr.sin_family = AF_INET;
        peer_addr.sin_addr.s_addr = inet_addr(peer_ip.c_str());
        peer_addr.sin_port = htons(peer_port);
        ret = platform_udp_send_data(original_receiver_fd, msg, message_length, &peer_addr);
    }

    midLog("Sent Response via FD:%d", original_receiver_fd);
    return ret;
}

int platform_sctp_send_data(int sending_fd, std::vector<char>& msg, int message_length,
    struct sockaddr_in *sin, uint16_t stream_id
    ) {
    int rc = sctp_sendmsg(sending_fd, msg.data(), message_length, (sockaddr *)sin, sin ? sizeof(struct sockaddr_in) : 0, SCTP_PPID_NGAP, 0, stream_id, 0, 0);
    if (rc == -1) {
        higLog("sendto failed, Error: %s", strerror(errno));

    } else {
        midLog("Sent %d bytes on stream %d", rc, stream_id);
    }
    return rc;
}

int platform_tcp_send_data(int sending_fd, std::vector<char>& msg, int message_length, 
    struct sockaddr_in *sin
    ) {
    int rc = sendto(sending_fd, msg.data(), message_length, 0, (sockaddr *)sin, sin ? sizeof(struct sockaddr_in) : 0);
    if (rc == -1) {
        lowLog("sendto failed, Error: %s", strerror(errno));
    }
    midLog("Sent %d bytes", rc);
    return rc; 
}


int platform_udp_send_data(int sending_fd, std::vector<char>& msg, int message_length, 
    struct sockaddr_in *sin
    ) {
    lowLog("Sending UDP data\n");
    // lowLog("port: %d\n", ntohs(sin->sin_port));
    // lowLog("sending_fd: %d\n", sending_fd);
    int rc = sendto(sending_fd, msg.data(), message_length, 0, (sockaddr *)sin,
                sin ? sizeof(struct sockaddr_in) : 0);
    if (rc == -1) {
        lowLog("sendto failed, Error: %s\n", strerror(errno));
    }
    lowLog("Sent %d bytes\n", rc);
    return rc;
}

int send_new_request(std::string peer_ip, int peer_port, std::vector<char>& msg, _e_connectionType conn_type, 
    _e_protocols protocol, NODE peer_node, int message_length, int procedure_key,
    void (*callback)(std::vector<char>&, int, int, nfvInstanceData *), struct nfvInstanceData *nfvInst
    ) {
    (void) peer_node;
    bool is_new_sending_socket = false;
    
    int sending_socket_fd;

    struct sockaddr_in peer_addr {};
    peer_addr.sin_family = AF_INET;
    peer_addr.sin_addr.s_addr = inet_addr(peer_ip.c_str());
    peer_addr.sin_port = htons(peer_port);

    if (conn_type == SHORT) {
        // newly created data socket must be removed in next iteration of event loop
        sending_socket_fd = create_connection(peer_ip, peer_port, peer_addr, protocol);
        nfvInst->fd_to_be_closed_set.insert(sending_socket_fd);
        is_new_sending_socket = true;
    } else if (conn_type == PERSISTENT) {
        // check if the persistent connection socket already exists, if not
        // create one.
        int persistent_conn_identifier = (protocol << 8) | peer_node;
        pthread_mutex_lock(&persistent_conn_key_to_fd_map_lock);
        if (persistent_conn_key_to_fd_map.find(persistent_conn_identifier) != persistent_conn_key_to_fd_map.end()) {
            // connection was created earlier between this NF and peer NF
            // using the protocol specified by send_data() via pyramis SEND()
            sending_socket_fd = persistent_conn_key_to_fd_map[persistent_conn_identifier];
            pthread_mutex_unlock(&persistent_conn_key_to_fd_map_lock);
            
        } else {
            // first ever long connection requested between this nf and
            // peer nf.
            sending_socket_fd = create_connection(peer_ip, peer_port, peer_addr, protocol);
            
            // cache connection details
            persistent_conn_key_to_fd_map[persistent_conn_identifier] = sending_socket_fd;
            pthread_mutex_unlock(&persistent_conn_key_to_fd_map_lock);

            is_new_sending_socket = true;
        }

    }

    if (is_new_sending_socket) {
        fdData_t new_sending_socket {};
        new_sending_socket.fd = sending_socket_fd;
        if (protocol == SCTP_PROTOCOL)
            new_sending_socket.type = SCTP_PROTOCOL_CLIENT_SOCKET;
        else if (protocol == TCP_PROTOCOL)
            new_sending_socket.type = TCP_PROTOCOL_CLIENT_SOCKET;
        else if (protocol == UDP_PROTOCOL)
            new_sending_socket.type = UDP_PROTOCOL_CLIENT_SOCKET;

        // response expected on the newly created socket.
        nfvInst->fd_map[sending_socket_fd] = new_sending_socket;
        
        // add to epoll, will be detected in next iteration of 
        // epoll_wait() event loop
        struct epoll_event expecting_event {};
        expecting_event.events = EPOLLIN;
        expecting_event.data.fd = sending_socket_fd;
        int epfd = nfvInst->epoll_fd;

        if (epoll_ctl(epfd, EPOLL_CTL_ADD, sending_socket_fd, &expecting_event) == -1)
        {
            higLog("epoll_ctl failed, fd id %d, Error %s", fdd.id, strerror(errno));
            LOG_EXIT;
            return -1;
        }
        midLog("Fd:%d added to epoll", fd);

        // message received on fd will correspond to a response, hence
        // a response_handler is registered. On next epoll wait iteration,
        // the response handler will be called.
        pthread_mutex_lock(&fd_response_handler_map_lock);
        fd_response_handler_map[sending_socket_fd] = callback;
        
        // the thread that updated the response_handler also needs
        // to map the new fd to procedure_key (for when app needs to get procedure_key
        // from the sockfd that recd the current message)
        fd_to_key_map[sending_socket_fd] = procedure_key;
        pthread_mutex_lock(&fd_response_handler_map_lock);         
    }
    int ret;
    if (protocol == SCTP_PROTOCOL)
        ret = platform_sctp_send_data(sending_socket_fd, msg, message_length, &peer_addr, 0);
    else if (protocol == TCP_PROTOCOL)
        ret = platform_tcp_send_data(sending_socket_fd, msg, message_length, &peer_addr);
    else if (protocol == UDP_PROTOCOL)
        ret = platform_udp_send_data(sending_socket_fd, msg, message_length, &peer_addr);

    if (is_new_sending_socket) {
        lowLog("Sent request from new socket fd: %d\n", sending_socket_fd);
    } else {
        lowLog("Sent request from previously created socket fd: %d\n", sending_socket_fd);
    }

    return ret;   
}

int create_connection(std::string peer_ip, int peer_port, struct sockaddr_in peer_addr, _e_protocols protocol) {
    (void) peer_port, (void) peer_ip;
    int new_data_socket_fd = 0;
	if (protocol == TCP_PROTOCOL)
        new_data_socket_fd = socket(AF_INET, SOCK_STREAM, 0);
    else if (protocol == UDP_PROTOCOL)
        new_data_socket_fd = socket(AF_INET, SOCK_DGRAM, 0);
    else if (protocol == SCTP_PROTOCOL)
        new_data_socket_fd = socket(AF_INET, SOCK_STREAM, IPPROTO_SCTP);
    else {
        higLog("Protocol is not supported\n");
    }

    // the NF acts as a client, allocates random port and new socket 
    // to act as the connection endpoint for peer i.e. server it
    // is wants to connect with.
    // ----------------------------------------------------------------
    // connect() on a udp socket binds a peer (dest) addr to the socket.
    // and filters incoming packets based on their source addr.
    // Thus subsequent message send do no need a destination addr if 
    // the same socket is used.
    int ret = connect(new_data_socket_fd, (struct sockaddr *)&peer_addr, sizeof(struct sockaddr_in));
    if (ret < 0) {
        if (errno != EINPROGRESS) {
			perror("CONNECT FAILED");
			close(new_data_socket_fd);
            LOG_EXIT;
			return -1;
		}
	}
    return new_data_socket_fd;
}

int platform_tcp_recv_data(int active_fd, std::vector<char>& msg) {
  LOG_ENTRY;
    int rc;
    std::vector<char> buffer(MAX_MESSAGE_SIZE);
    int cnt = 0;

    for(;;) {
        // Attempt to read up to MAX_MESSAGE_SIZE bytes from the fd
        rc = recv(active_fd, buffer.data(), MAX_MESSAGE_SIZE, 0);
        if (rc > 0) {
            // Append received data to msg
            msg.insert(msg.end(), buffer.begin(), buffer.begin() + rc);
            cnt += rc;
            // Check termination conditions
            if (msg[cnt - 1] == '}' ||
                (cnt >= 4 && msg[cnt - 1] == 10 && msg[cnt - 2] == 13 && msg[cnt - 3] == 45 && msg[cnt - 4] == 45) ||
                msg[cnt - 1] == ']' ||
                (cnt >= 21 && std::equal(msg.end() - 21, msg.end(), "Content-Length: 0\r\n\r\n"))) {
                break;
            }
        } else if (rc == 0) {
            return 0; // Connection closed by the peer
        } else {
            lowLog("recv failed, Error: %s", strerror(errno));
            return rc;
        }
    }

    LOG_EXIT;
    return cnt;
}

int platform_udp_recv_data(int active_fd, std::vector<char>& msg) {
    LOG_ENTRY;

    // Receive udp datagram
    int rc = recvfrom(active_fd, msg.data(), MAX_MESSAGE_SIZE, 0, NULL, NULL);
    
    if (rc == -1) {
        lowLog("recvfrom failed, Error: %s", strerror(errno));
    } else if (rc == 0) {
        lowLog("%s", "recvfrom returned 0 len msg");
    } else {
        // Resize the vector to the actual size of the received data
        msg.resize(rc);
    }

    LOG_EXIT;
    return rc;
}

int platform_sctp_recv_data(int active_fd, std::vector<char>& msg, int *stream_id) {
    LOG_ENTRY;
    struct sctp_sndrcvinfo info = {};
    int flags = 0;

    // Receive data
    int rc = sctp_recvmsg(active_fd, msg.data(), MAX_MESSAGE_SIZE, NULL, NULL, &info, &flags);
    
    if (rc < 0) {
        lowLog("recvfrom failed, Error: %s", strerror(errno));
        LOG_EXIT;
        if (errno == ECONNRESET) { // Non-graceful shutdown
            return 0;
        }
        return rc;
    } else if (rc == 0) { // Graceful shutdown
        lowLog("%s", "recvfrom returned 0 len msg");
        LOG_EXIT;
        return rc;
    } else if (flags & MSG_NOTIFICATION) {
        // Received a notification, not handling it.
        LOG_EXIT;
        return -1;
    }

    // Resize the vector to the actual size of the received data
    msg.resize(rc);

    *stream_id = info.sinfo_stream;
    if (info.sinfo_ppid != SCTP_PPID_NGAP) {
        if (ppid_warn_rate++ % 20 == 0) {
            midLog("RATELIMITED_LOG: Warning: PPID is not 60, recv.d %d", ntohl(info.sinfo_ppid));
            // We don't need strict rate limited logging, which is why
            // we are not using locks to protect ppid_warn_rate
        }
    }
    LOG_EXIT;
    return rc;
}

int main(int argc, char *argv[]) {
    //setTag(argv[0]); // open log file with custom filename

    if (argc != 3) {
        std::cerr << "Usage: ./server lo #threads" << "\n";
        return 0;
    }

    int num_threads = stoi(argv[2]);

    // get local_addr of NF
    std::string ip = "127.0.0.1";
    uint32_t bind_addr = INADDR_ANY;

    if (inet_pton(AF_INET, ip.c_str(), &bind_addr) < 0)
    {
        lowLog("ipnet_pton failed, %s\n", strerror(errno));
        exit(1);
    }

    // setup global config stuff
    // One nfvInst per thread.
    nfvInstVector.resize(num_threads);
    for (int i = 0; i < num_threads; i++) {
        nfvInstVector[i] = init_platform(bind_addr);
        nfvInstVector[i]->id = i;
    }

    // Register request handler
    port_request_handler_map[5858] = server_entry;

    // create listen sockets for interfaces of one NF instance, copy
    // contents to all NF instances. More specifically, copy the fd
    // of the interface listening sockets to each NF instance.
    // UDP sockets persist.
    open_channels(nfvInstVector[0]);
    for (int i = 1; i < num_threads; i++) {
        for (auto fd_map_itr = nfvInstVector[0]->fd_map.begin(); fd_map_itr != nfvInstVector[0]->fd_map.end(); fd_map_itr++) {
                nfvInstVector[i]->fd_map[fd_map_itr->first] = fd_map_itr->second;
        }
    }

    // create threads.
    // each thread performs blocking epoll_wait on the same 
    // fd i.e. sockets at server init.
    std::vector<pthread_t> threads(num_threads);
    for (int i = 0; i < num_threads; i++) {
        nfvInstVector[i]->thread_id = i;
        if (pthread_create(&threads[i], NULL, poll_on_events, (void *)nfvInstVector[i]) < 0) {
            lowLog("pthread failed\n");
            return 0;
        }
    }

    for (int i = 0; i < num_threads; i++) {
        pthread_join(threads[i], NULL);
        midLog("Thread %d joined", i);
    }

    return 0;
}